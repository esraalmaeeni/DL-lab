# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xMxEEpd_EGLzSOMslWViNyD4OQjt8xg
"""

pip install torch scikit-learn torchtext

pip install torch==2.1.0 torchtext==0.16.0

pip uninstall torch torchtext -y

pip install torch==2.1.0+cu118 torchtext==0.16.0 -f https://download.pytorch.org/whl/torch_stable.html

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from torchtext.data.metrics import bleu_score
from sklearn.model_selection import train_test_split
from collections import Counter
from torch.nn.utils.rnn import pad_sequence
import re

# Load data
with open('dataset.pdf', encoding='utf-8') as f:
    lines = f.readlines()

pairs = [line.strip().lower().split('\t') for line in lines if '\t' in line]
pairs = pairs[:10000]  # subsample

# Tokenization
def tokenize(text):
    return re.findall(r"\b\w+\b", text)

eng_sentences = [tokenize(pair[0]) for pair in pairs]
spa_sentences = [['<sos>'] + tokenize(pair[1]) + ['<eos>'] for pair in pairs]

# Vocabulary
def build_vocab(sentences):
    counter = Counter(word for sent in sentences for word in sent)
    vocab = {'<pad>': 0, '<unk>': 1}
    vocab.update({word: i + 2 for i, word in enumerate(counter)})
    return vocab

eng_vocab = build_vocab(eng_sentences)
spa_vocab = build_vocab(spa_sentences)
eng_ivocab = {i: w for w, i in eng_vocab.items()}
spa_ivocab = {i: w for w, i in spa_vocab.items()}

# Numericalize
def encode(sentences, vocab):
    return [[vocab.get(word, vocab['<unk>']) for word in sent] for sent in sentences]

eng_data = encode(eng_sentences, eng_vocab)
spa_data = encode(spa_sentences, spa_vocab)

# Padding
def collate_fn(batch):
    eng_batch, spa_batch = zip(*batch)
    eng_batch = pad_sequence([torch.tensor(x) for x in eng_batch], padding_value=0, batch_first=True)
    spa_batch = pad_sequence([torch.tensor(x) for x in spa_batch], padding_value=0, batch_first=True)
    return eng_batch, spa_batch

# Dataset + Split
data = list(zip(eng_data, spa_data))
train_data, temp = train_test_split(data, test_size=0.2)
val_data, test_data = train_test_split(temp, test_size=0.5)

from torch.utils.data import DataLoader
train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_data, batch_size=64, collate_fn=collate_fn)

# Seq2Seq Model
class Encoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)

    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim, vocab_size)

    def forward(self, input, hidden, cell):
        embedded = self.embedding(input.unsqueeze(1))
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(1))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len = trg.shape
        trg_vocab_size = self.decoder.fc_out.out_features
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        hidden, cell = self.encoder(src)
        input = trg[:, 0]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t] = output
            top1 = output.argmax(1)
            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1
        return outputs

# Initialize
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
INPUT_DIM = len(eng_vocab)
OUTPUT_DIM = len(spa_vocab)
EMB_DIM = 256
HID_DIM = 512

enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)
dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)
model = Seq2Seq(enc, dec, device).to(device)

# Training
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=0)

def train(model, loader):
    model.train()
    epoch_loss = 0
    for src, trg in loader:
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()
        output = model(src, trg)
        output = output[:, 1:].reshape(-1, output.shape[-1])
        trg = trg[:, 1:].reshape(-1)
        loss = criterion(output, trg)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(loader)

for epoch in range(10):
    loss = train(model, train_loader)
    print(f"Epoch {epoch+1} | Loss: {loss:.4f}")

# Evaluation
def translate(model, sentence):
    model.eval()
    tokens = tokenize(sentence.lower())
    indexed = [eng_vocab.get(t, eng_vocab['<unk>']) for t in tokens]
    src_tensor = torch.tensor(indexed).unsqueeze(0).to(device)
    hidden, cell = model.encoder(src_tensor)
    input = torch.tensor([spa_vocab['<sos>']]).to(device)
    result = []
    for _ in range(20):
        output, hidden, cell = model.decoder(input, hidden, cell)
        top1 = output.argmax(1).item()
        if top1 == spa_vocab['<eos>']:
            break
        result.append(spa_ivocab[top1])
        input = torch.tensor([top1]).to(device)
    return ' '.join(result)

# BLEU Score
def evaluate_bleu(model, data):
    trgs, preds = [], []
    for src, trg in data[:100]:
        src_text = ' '.join([eng_ivocab.get(i, '') for i in src])
        trg_text = [spa_ivocab.get(i, '') for i in trg if i not in {0, 1}]
        pred_text = translate(model, src_text).split()
        trgs.append([trg_text[1:-1]])
        preds.append(pred_text)
    return bleu_score(preds, trgs)

print(f"BLEU Score: {evaluate_bleu(model, test_data) * 100:.2f}")

