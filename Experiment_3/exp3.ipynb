{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmtX+DRKLJ/vi5Dq9tsYLv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esraalmaeeni/DL-lab/blob/main/exp3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suR-oEGNO7PI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation='relu', input_size=32):\n",
        "        \"\"\"\n",
        "        num_classes: Number of output classes.\n",
        "        activation: 'relu', 'tanh', or 'leaky_relu'.\n",
        "        input_size: Spatial size of the input image (e.g. 32 for CIFARâ€‘10, 64 for Cats vs. Dogs).\n",
        "        \"\"\"\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        # Select the activation function\n",
        "        if activation.lower() == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif activation.lower() == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        elif activation.lower() == 'leaky_relu':\n",
        "            self.act = nn.LeakyReLU(negative_slope=0.1)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function: choose 'relu', 'tanh', or 'leaky_relu'\")\n",
        "\n",
        "        # First Convolutional Block\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            self.act,\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        # Second Convolutional Block\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            self.act,\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        # Third Convolutional Block\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            self.act,\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        # Determine final feature map size (after 3 poolings, each dividing by 2)\n",
        "        feature_map_size = input_size // 8\n",
        "        self.feature_size = 128 * feature_map_size * feature_map_size\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 256),\n",
        "            self.act,\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "r1vxpjc7WZuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(model, init_type='xavier'):\n",
        "    \"\"\"\n",
        "    init_type: 'xavier', 'kaiming', or 'random'\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            if init_type.lower() == 'xavier':\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            elif init_type.lower() == 'kaiming':\n",
        "                # Note: If your activation is Tanh, Kaiming may be less optimal.\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "            elif init_type.lower() == 'random':\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.02)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported initialization type\")\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ],
      "metadata": {
        "id": "fwfJ4SbFWgRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, optimizer_name='adam', lr=1e-3):\n",
        "    if optimizer_name.lower() == 'sgd':\n",
        "        return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif optimizer_name.lower() == 'adam':\n",
        "        return optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optimizer_name.lower() == 'rmsprop':\n",
        "        return optim.RMSprop(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer\")\n"
      ],
      "metadata": {
        "id": "Aw_HF3V-WmRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += torch.sum(preds == labels).item()\n",
        "        total += inputs.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels).item()\n",
        "            total += inputs.size(0)\n",
        "    return running_loss / total, correct / total\n"
      ],
      "metadata": {
        "id": "YfVsPleMWpNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for CIFAR-10\n",
        "transform_cifar_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "transform_cifar_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "cifar_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar_train)\n",
        "cifar_testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar_test)\n",
        "\n",
        "trainloader_cifar = DataLoader(cifar_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader_cifar  = DataLoader(cifar_testset, batch_size=128, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ3DbPE5WqPY",
        "outputId": "45fd415e-3ba6-4659-9f4b-45a1582b84d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for Cats vs. Dogs (resized to 64x64)\n",
        "transform_catsdogs = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Folder structure: ./data/cats_vs_dogs/train and ./data/cats_vs_dogs/val\n",
        "catsdogs_trainset = datasets.ImageFolder(os.path.join('./data/cats_vs_dogs', 'train'), transform=transform_catsdogs)\n",
        "catsdogs_valset   = datasets.ImageFolder(os.path.join('./data/cats_vs_dogs', 'val'), transform=transform_catsdogs)\n",
        "\n",
        "trainloader_catsdogs = DataLoader(catsdogs_trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valloader_catsdogs   = DataLoader(catsdogs_valset, batch_size=32, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "XX9X5L7VWvYH",
        "outputId": "0a761dc8-837d-4eac-e0ec-c3f2eab90d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/cats_vs_dogs/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2e2ada4a981e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Folder structure: ./data/cats_vs_dogs/train and ./data/cats_vs_dogs/val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcatsdogs_trainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/cats_vs_dogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_catsdogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcatsdogs_valset\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/cats_vs_dogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_catsdogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cats_vs_dogs/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter configurations\n",
        "activations = ['relu', 'tanh', 'leaky_relu']\n",
        "init_methods = ['xavier', 'kaiming', 'random']\n",
        "optimizers_list = ['sgd', 'adam', 'rmsprop']\n",
        "\n",
        "num_epochs = 10       # Adjust as needed\n",
        "learning_rate = 1e-3\n",
        "num_classes = 10      # For CIFAR-10\n",
        "input_size = 32       # CIFAR-10 images are 32x32\n",
        "\n",
        "best_overall_acc = 0.0\n",
        "best_config = {}\n",
        "best_model_state = None\n",
        "\n",
        "for act in activations:\n",
        "    for init_method in init_methods:\n",
        "        for opt_name in optimizers_list:\n",
        "            print(f\"\\n=== Training with Activation: {act}, Initialization: {init_method}, Optimizer: {opt_name} ===\")\n",
        "\n",
        "            # Initialize the model\n",
        "            model = CustomCNN(num_classes=num_classes, activation=act, input_size=input_size).to(device)\n",
        "            initialize_weights(model, init_type=init_method)\n",
        "\n",
        "            # Setup optimizer and loss criterion\n",
        "            optimizer = get_optimizer(model, optimizer_name=opt_name, lr=learning_rate)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            best_val_acc = 0.0\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            # Training loop\n",
        "            for epoch in range(num_epochs):\n",
        "                start_time = time.time()\n",
        "                train_loss, train_acc = train_epoch(model, trainloader_cifar, optimizer, criterion, device)\n",
        "                val_loss, val_acc = evaluate(model, testloader_cifar, criterion, device)\n",
        "                elapsed = time.time() - start_time\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "                # Save model weights if validation accuracy improves\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print(f\"Best validation accuracy for current configuration: {best_val_acc:.4f}\")\n",
        "\n",
        "            # Check if this configuration is the best overall\n",
        "            if best_val_acc > best_overall_acc:\n",
        "                best_overall_acc = best_val_acc\n",
        "                best_config = {'activation': act, 'init': init_method, 'optimizer': opt_name}\n",
        "                best_model_state = best_model_wts\n",
        "\n",
        "print(\"\\n=== Best Overall Configuration on CIFAR-10 ===\")\n",
        "print(best_config)\n",
        "print(f\"Validation Accuracy: {best_overall_acc:.4f}\")\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_state, 'best_cifar_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDLcs_oEWzPq",
        "outputId": "40718d31-4989-4e70-9f2e-9685f9bff29b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training with Activation: relu, Initialization: xavier, Optimizer: sgd ===\n",
            "Epoch 1/10 - Train Loss: 2.1013, Train Acc: 0.2440 | Val Loss: 1.7742, Val Acc: 0.3754 | Time: 158.67s\n",
            "Epoch 2/10 - Train Loss: 1.7666, Train Acc: 0.3442 | Val Loss: 1.5714, Val Acc: 0.4361 | Time: 150.59s\n",
            "Epoch 3/10 - Train Loss: 1.6409, Train Acc: 0.3941 | Val Loss: 1.4825, Val Acc: 0.4658 | Time: 149.26s\n",
            "Epoch 4/10 - Train Loss: 1.5740, Train Acc: 0.4155 | Val Loss: 1.4071, Val Acc: 0.4985 | Time: 149.05s\n",
            "Epoch 5/10 - Train Loss: 1.5138, Train Acc: 0.4443 | Val Loss: 1.3744, Val Acc: 0.5053 | Time: 153.63s\n",
            "Epoch 6/10 - Train Loss: 1.4716, Train Acc: 0.4608 | Val Loss: 1.3303, Val Acc: 0.5236 | Time: 150.82s\n",
            "Epoch 7/10 - Train Loss: 1.4374, Train Acc: 0.4725 | Val Loss: 1.2668, Val Acc: 0.5462 | Time: 149.23s\n",
            "Epoch 8/10 - Train Loss: 1.3964, Train Acc: 0.4911 | Val Loss: 1.2443, Val Acc: 0.5512 | Time: 151.26s\n",
            "Epoch 9/10 - Train Loss: 1.3720, Train Acc: 0.5039 | Val Loss: 1.2422, Val Acc: 0.5507 | Time: 149.14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ResNet-18\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "in_features = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(in_features, num_classes)  # Adjust output layer\n",
        "resnet18 = resnet18.to(device)\n",
        "\n",
        "# Option: Fine-tune all layers (or freeze some layers as needed)\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Setup optimizer and loss\n",
        "optimizer_resnet = get_optimizer(resnet18, optimizer_name='adam', lr=learning_rate)\n",
        "criterion_resnet = nn.CrossEntropyLoss()\n",
        "\n",
        "best_resnet_acc = 0.0\n",
        "best_resnet_wts = copy.deepcopy(resnet18.state_dict())\n",
        "\n",
        "print(\"\\n=== Fine-tuning Pretrained ResNet-18 ===\")\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_epoch(resnet18, trainloader_cifar, optimizer_resnet, criterion_resnet, device)\n",
        "    val_loss, val_acc = evaluate(resnet18, testloader_cifar, criterion_resnet, device)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"ResNet Epoch {epoch+1}/{num_epochs} - \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "    if val_acc > best_resnet_acc:\n",
        "        best_resnet_acc = val_acc\n",
        "        best_resnet_wts = copy.deepcopy(resnet18.state_dict())\n",
        "\n",
        "print(f\"Best ResNet-18 Validation Accuracy: {best_resnet_acc:.4f}\")\n",
        "torch.save(best_resnet_wts, 'best_resnet_model.pth')\n"
      ],
      "metadata": {
        "id": "zhE8AvmoW9Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4O-HPUFXAlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
